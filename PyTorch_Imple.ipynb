{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Data Preprocessing  --------'''\n",
    "\n",
    "'''-------- Cleaning of Sentences --------'''\n",
    "\n",
    "def funtuationMarksSep (Sentence):    \n",
    "     \n",
    "    strippedSen = Sentence.lstrip().rstrip()\n",
    "    dots =[i for i, letter in enumerate(strippedSen) if letter == '.']\n",
    "    \n",
    "    if (len(dots)) > 0:\n",
    "        strippedSen = strippedSen.replace('.', '')\n",
    "        strippedSen = strippedSen + ' .'\n",
    "    else:\n",
    "        strippedSen = strippedSen + ' .'    \n",
    "    \n",
    "    commas = [i for i, letter in enumerate(strippedSen) if letter == ',']\n",
    "    if (len(commas)) > 0:\n",
    "        sen = []\n",
    "\n",
    "        for i in range (len(commas)):\n",
    "            if ( i == 0):\n",
    "                sen.append(strippedSen[:(commas[i])])\n",
    "            else:\n",
    "                sen.append(strippedSen[(commas[i-1]):(commas[i])])\n",
    "        \n",
    "            if (i == (len(commas)) -1):\n",
    "                sen.append(strippedSen[(commas[i]):])    \n",
    "\n",
    "        sentence = ''\n",
    "        \n",
    "        for i in range (len(sen)):\n",
    "            pos = sen[i].find (',')    \n",
    "    \n",
    "            if (pos > 0):\n",
    "                sen[i] = sen[i][:pos]       \n",
    "                sentence = sentence.lstrip().rstrip() + ' '+ sen[i].lstrip().rstrip()   \n",
    "                \n",
    "            elif (pos == 0):\n",
    "                sen[i] = sen[i][pos+1:]         \n",
    "                sentence =  sentence.lstrip().rstrip() + ' , '+ sen[i].lstrip().rstrip() \n",
    "                \n",
    "            else:\n",
    "                if not (sen[i] == ' '):\n",
    "                    sentence = sentence.lstrip().rstrip()+ ' ' + sen[i].lstrip().rstrip()\n",
    "                    \n",
    "    else:\n",
    "        sentence = strippedSen\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Data Preprocessing  --------'''\n",
    "\n",
    "'''-------- Reading & Loading the Synonyms --------'''\n",
    "\n",
    "def synonymWordLoading ():\n",
    "    \n",
    "    synonymWords = []\n",
    "    synonymWordFile = \"similarWords.txt\"\n",
    "\n",
    "    with open(synonymWordFile,\"r\") as data:\n",
    "    \n",
    "        for line in data:\n",
    "            words = line.split('\\t') \n",
    "            wordList = []    \n",
    "            lngth = len(words)\n",
    "                \n",
    "            if (words[lngth - 1] == '\\n') :        \n",
    "            \n",
    "                words.pop(0)            \n",
    "                length = len(words)\n",
    "                words.pop(length-1)\n",
    "                words.pop(length-2)\n",
    "                words.remove('-')\n",
    "                wordList.append(words[0].lstrip().rstrip())\n",
    "            \n",
    "                if (len(words) == 1):\n",
    "                    wordList.append(words[0])\n",
    "                else:\n",
    "                    newWord = words[1].split(',')           \n",
    "            \n",
    "                    for i in range (len(newWord)):\n",
    "                        wordList.append(newWord[i].lstrip().rstrip()) \n",
    "            else :\n",
    "                words.pop(0)\n",
    "                words.remove('-') \n",
    "                wordList.append(words[0].lstrip().rstrip())\n",
    "                words = words[1].replace('\\n', '')   \n",
    "            \n",
    "                if (len(words) == 1):                \n",
    "                    wordList.append(words[0])\n",
    "                else:\n",
    "                    newWord = words.split(',')           \n",
    "            \n",
    "                    for i in range (len(newWord)):                    \n",
    "                        wordList.append(newWord[i].lstrip().rstrip())            \n",
    "            \n",
    "            synonymWords.append(wordList)\n",
    "            \n",
    "    return synonymWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Data Preprocessing  --------'''\n",
    "\n",
    "'''-------- Replacing Synonyms --------'''\n",
    "\n",
    "def synonymWordReplacement(Sentence,synonymWords):\n",
    "    \n",
    "    newWords = []\n",
    "    \n",
    "    words = Sentence.split(\" \") \n",
    "    sen = ''\n",
    "    \n",
    "    for i in range (len(words)):\n",
    "        wordExsitence = True         \n",
    "                \n",
    "        for j in range (len(synonymWords)):\n",
    "            synonymList = synonymWords[j]  \n",
    "                                               \n",
    "            if ((words[i].lstrip().rstrip()) == synonymList[0] ):                 \n",
    "                newWords.append(synonymList[0])                               \n",
    "                wordExsitence = False\n",
    "                break\n",
    "                \n",
    "            elif (words[i].lstrip().rstrip()) in synonymList[1:(len(synonymList))]:                \n",
    "                newWords.append(synonymList[0])\n",
    "                wordExsitence = False\n",
    "                break\n",
    "                        \n",
    "        if (wordExsitence):\n",
    "            newWords.append(words[i].lstrip().rstrip()) \n",
    "    \n",
    "    newSen = ''\n",
    "    \n",
    "    for i in range (len (newWords)) :\n",
    "        if (i == 0):\n",
    "            newSen = newWords[i] + \" \"\n",
    "        else :\n",
    "            newSen = newSen + newWords[i] + \" \" \n",
    "    \n",
    "                \n",
    "    return newSen, newWords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Data Preprocessing  --------'''\n",
    "\n",
    "'''-------- Stemming Of Words --------'''\n",
    "\n",
    "def wordStemming(sentence, wordVector):\n",
    "            \n",
    "    rows = []\n",
    "    words = []    \n",
    "    \n",
    "    for token in sentence:\n",
    "        i = len(token)\n",
    "        \n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            \n",
    "            if word in wordVector:  \n",
    "                \n",
    "                rows.append(wordVector[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "                \n",
    "    return rows, words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Data Preprocessing --------'''\n",
    "\n",
    "'''-------- Data Preprocessing Function--------'''\n",
    "\n",
    "def TrainDataPreProcessing (synonymWords, fileName):\n",
    "    \n",
    "    eviSentences = []\n",
    "    hypSentences = []\n",
    "    labels = []\n",
    "    #scores = [] \n",
    "    DataSet = []\n",
    "    cnt = 0\n",
    "        \n",
    "    with open(fileName,\"r\") as data:\n",
    "        for line in data:  \n",
    "            cnt =  cnt + 1    \n",
    "            records = line.split('|') \n",
    "            \n",
    "            if (len(records) > 1):                \n",
    "                Sentence = []\n",
    "                cleanedEviSentence = funtuationMarksSep(records[1].lstrip().rstrip())\n",
    "                replacedEviSentence, eviSen = synonymWordReplacement ((cleanedEviSentence.lstrip().rstrip()), synonymWords)               \n",
    "                eviSentences.append(eviSen )               \n",
    "                \n",
    "                cleanedHypSentence = funtuationMarksSep(records[2].lstrip().rstrip())                \n",
    "                replacedHypSentence, hypSen = synonymWordReplacement ((cleanedHypSentence.lstrip().rstrip()), synonymWords) \n",
    "                hypSentences.append(hypSen)                \n",
    "                \n",
    "                labels.append((records[0].lstrip()).rstrip())                \n",
    "                #scores.append(scoreSetup(records[3].lstrip().rstrip())) \n",
    "                Sentence.append(replacedEviSentence)\n",
    "                Sentence.append(replacedHypSentence)\n",
    "                Sentence.append((records[0].lstrip()).rstrip())\n",
    "                                \n",
    "                DataSet.append (Sentence)   \n",
    "              \n",
    "        return DataSet, eviSentences, hypSentences, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11360\n",
      "1245\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "'''-------- Data Preprocessing --------'''\n",
    "\n",
    "'''-------- Data Loading & Preprocessing --------'''\n",
    "\n",
    "trainDataFileName = \"Data/trainData.txt\"\n",
    "valDataFileName = \"Data/valData.txt\"\n",
    "testDataFileName = \"Data/testData.txt\"\n",
    "\n",
    "synonymWords = synonymWordLoading ()\n",
    "\n",
    "processedTrainDataSet, trainEviSen, trainHypSen, trainLabels = TrainDataPreProcessing (synonymWords, trainDataFileName)\n",
    "processedValidationDataSet, valEviSen, valHypSen, valLabels = TrainDataPreProcessing (synonymWords, valDataFileName)\n",
    "processedTestDataSet, testEviSen, testHypSen, testLabels = TrainDataPreProcessing (synonymWords, testDataFileName)\n",
    "\n",
    "print (len(processedTrainDataSet))\n",
    "print (len(processedValidationDataSet))\n",
    "print (len(processedTestDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Imports--------'''\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n",
      "<class 'torchtext.data.field.LabelField'>\n"
     ]
    }
   ],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Toeknizer --------'''\n",
    "\n",
    "\n",
    "Toeknizer = spacy.load('en')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in Toeknizer.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, batch_first= True)\n",
    "print (type(TEXT))\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.long, batch_first= True)\n",
    "print (type(LABEL))\n",
    "\n",
    "Splitter = [('Evi',TEXT), ('Hyp',TEXT),('Label',LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.dataset.Dataset'>\n",
      "{'Evi': ['ප්\\u200dරෝටීන', 'යනු', 'ඇමයිනෝ', 'ඇඹුල්', 'දාම', 'එකකින්', 'හෝ', 'වැඩි', 'ගණනකින්', 'සැදුම්ලත්', 'විශාල', 'කාබනික', 'සංයෝගයන්ය', '.'], 'Hyp': ['යනු', 'හෝ', 'සංයෝගයන්ය', 'සැදුම්ලත්', 'වැඩි', 'ඇමයිනෝ', 'දාම', 'ප්\\u200dරෝටීන', 'කාබනික', 'එකකින්', 'එකම', '.'], 'Label': 'P'}\n",
      "11360\n",
      "\n",
      "\n",
      "<class 'torchtext.data.dataset.Dataset'>\n",
      "{'Evi': ['එමගින්', 'කලාපයේ', 'ජනයා', 'ගැටුම්', 'වලින්', 'තොර', 'මනා', 'කලා', 'රසඥතාවයකින්', 'හෙබි', 'සාමකාමී', 'අය', 'වුහ', '.'], 'Hyp': ['වලින්', 'මනා', 'වුහ', 'හෙබි', 'කලා', 'අය', 'කලාපයේ', 'තොර', 'සාමකාමී', 'රසඥතාවයකින්', 'ගැටුම්', 'ජනයා', 'එමගින්', 'එකඟ', '.'], 'Label': 'P'}\n",
      "1245\n",
      "\n",
      "\n",
      "<class 'torchtext.data.dataset.Dataset'>\n",
      "{'Evi': ['ශ්\\u200dරී', 'ලංකාව', 'දියුණු', 'වෙමින්', 'පවතින', 'තුන්වෙනි', 'ලෝකයේ', 'රටකි', '.'], 'Hyp': ['ශ්\\u200dරී', 'ලංකාව', 'නොදියුණු', 'තුන්වෙනි', 'ලෝකයේ', 'රටකි', '.'], 'Label': 'P'}\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Dataset Preparation --------'''\n",
    "\n",
    "Example = data.Example()\n",
    "trainSentences=[]\n",
    "validationSentences = []\n",
    "testSentences=[]\n",
    "\n",
    "for i in range (len(processedTrainDataSet)):\n",
    "    trainSentences.append(Example.fromlist(processedTrainDataSet[i], Splitter))\n",
    "    \n",
    "trainDS = data.Dataset(trainSentences, Splitter)\n",
    "\n",
    "for i in range (len(processedValidationDataSet)):\n",
    "    validationSentences.append(Example.fromlist(processedValidationDataSet[i], Splitter))\n",
    "    \n",
    "ValidationDS = data.Dataset(validationSentences, Splitter)\n",
    "\n",
    "for i in range (len(processedTestDataSet)):\n",
    "    testSentences.append(Example.fromlist(processedTestDataSet[i], Splitter))\n",
    "    \n",
    "testDS = data.Dataset(testSentences, Splitter)\n",
    "\n",
    "print (type(trainDS))\n",
    "print (vars(trainDS.examples[0]))\n",
    "print (len(trainDS))\n",
    "print (\"\\n\")\n",
    "\n",
    "print (type(ValidationDS))\n",
    "print (vars(ValidationDS.examples[0]))\n",
    "print (len(ValidationDS))\n",
    "print (\"\\n\")\n",
    "\n",
    "print (type(testDS))\n",
    "print (vars(testDS.examples[0]))\n",
    "print (len(testDS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Fast Text Embedding Loading --------'''\n",
    "\n",
    "#file needs to downloaded from https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "wordVectors = torchtext.vocab.Vectors('cc.si.300.vec', cache='Downloads/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5613\n",
      "[('.', 22720), ('සහ', 5422), (',', 5170), ('අතර', 2638), ('මෙම', 2444), ('ඇති', 2302), ('වන', 2198), ('වෙනස්', 2162), ('වගේ', 2154), ('ඇත', 1984), ('වේ', 1236), ('කරන', 1234), ('විශාල', 1184), ('එය', 1184), ('මද්\\u200dයස්ථ', 1180), ('මැද', 1172), ('පළමු', 1158), ('වැඩි', 1118), ('මැදහත්', 1112), ('වූ', 1052)]\n",
      "3\n",
      "[('C', 3796), ('P', 3784), ('N', 3780)]\n"
     ]
    }
   ],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Embedding Vocabulary --------'''\n",
    "\n",
    "\n",
    "TEXT.build_vocab(trainDS, vectors=wordVectors)\n",
    "LABEL.build_vocab(trainDS)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print (len(TEXT.vocab))\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "print (len(LABEL.vocab))\n",
    "print(LABEL.vocab.freqs.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Iterators --------'''\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (trainDS,ValidationDS,testDS),sort_key=lambda x: len(x.Evi), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-------- Neural Network --------'''\n",
    "\n",
    "'''-------- Neural Network Declarion --------'''\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.Rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first= True)\n",
    "        self.RNNFc = nn.Linear(hidden_dim*2, output_dim)        \n",
    "        self.finalFc = nn.Linear(hidden_dim*4, output_dim) \n",
    "        self.RELULayer = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def forward(self, EviSen, HypSen):\n",
    "        \n",
    "        eviEmbed = self.embedding(EviSen)        \n",
    "        hypEmbed = self.embedding(HypSen)    \n",
    "        \n",
    "        eviOutput, (eviHidden, eviCell) = self.Rnn(eviEmbed)\n",
    "        hypOutput, (hypHidden, hypCell) = self.Rnn(hypEmbed)\n",
    "        \n",
    "        hiddenEvi = torch.cat((eviHidden[-2,:,:], eviHidden[-1,:,:]), dim=1)\n",
    "        hiddenHyp = torch.cat((hypHidden[-2,:,:], hypHidden[-1,:,:]), dim=1)\n",
    "        \n",
    "                \n",
    "        combinedOutput = torch.cat((hiddenEvi,hiddenHyp),dim=1)        \n",
    "        finalOutPut = self.finalFc(combinedOutput)  \n",
    "        \n",
    "        return finalOutPut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5613, 300])\n",
      "RNN(\n",
      "  (embedding): Embedding(5613, 300)\n",
      "  (Rnn): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (RNNFc): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (finalFc): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (RELULayer): ReLU()\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters()) #Optimizer\n",
    "lossFun = nn.CrossEntropyLoss() #Loss Function\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = lossFun.to(device)\n",
    "\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(preds, correctLabels):    \n",
    "    _, predicted = torch.max(preds.data, 1)\n",
    "    correctPreds = 0\n",
    "    CPreds = 0\n",
    "    \n",
    "    \n",
    "    for i in range (len(predicted)):\n",
    "        if predicted[i] == correctLabels[i]:\n",
    "            CPreds  = CPreds  + 1\n",
    "           \n",
    "    #correctPreds += (predicted == correctLabels).sum().item()\n",
    "    \n",
    "    correctPreds = (predicted  == correctLabels).float() #convert into float for division \n",
    "    acc = correctPreds.sum()/len(correctPreds)\n",
    "    \n",
    "    #print ('CPreds is ' + str (CPreds))\n",
    "    #print ('Percentage :' + str (CPreds/ len(correctLabels)))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.Evi, batch.Hyp) \n",
    "        acc = Accuracy(predictions, batch.Label)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Label)               \n",
    "        loss.backward()   \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate (model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.Evi, batch.Hyp)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = Accuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 1.100 | Train Acc: 31.60% | Val. Loss: 1.095 | Val. Acc: 33.62% |\n",
      "| Epoch: 02 | Train Loss: 1.082 | Train Acc: 45.49% | Val. Loss: 0.831 | Val. Acc: 62.48% |\n",
      "| Epoch: 03 | Train Loss: 0.688 | Train Acc: 67.53% | Val. Loss: 0.593 | Val. Acc: 68.36% |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, validation_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = validate(model, validation_iterator, criterion)\n",
    "    #train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.870 | Test Acc: 56.18% |\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
